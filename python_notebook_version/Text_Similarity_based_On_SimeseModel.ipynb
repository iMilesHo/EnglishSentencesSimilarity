{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A Siamese Network-Based for similarity calculateing between two text\n"
   ],
   "metadata": {
    "id": "3ALndNqkDQ9Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Environment prepare"
   ],
   "metadata": {
    "id": "iuXnb9qcDh4e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dq8kD88DlQeJ",
    "outputId": "26d06e3c-4235-49c0-9f0d-91223f82e802"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import ssl\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "metadata": {
    "id": "LbDz0QP1mNRL"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# check if punkt tokenizer models are downloaded\n",
    "try:\n",
    "    find('tokenizers/punkt')\n",
    "    print(\"Punkt Tokenizer Models are already downloaded.\")\n",
    "except LookupError:\n",
    "    print(\"Punkt Tokenizer Models not found. Downloading them...\")\n",
    "    nltk.download('punkt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kEJNOYImHGx",
    "outputId": "0fd10b78-d065-4f66-8fe2-69425691e9fe"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Punkt Tokenizer Models not found. Downloading them...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model define"
   ],
   "metadata": {
    "id": "WV8upf4fDqwq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size=41699, d_model=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        # No need to explicitly define mean and normalization, can be done in forward\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Assuming x1 and x2 are the input sequences for the two siamese branches\n",
    "        x1 = self.embedding(x1)\n",
    "        x2 = self.embedding(x2)\n",
    "\n",
    "        # LSTM layer\n",
    "        x1, _ = self.lstm(x1)\n",
    "        x2, _ = self.lstm(x2)\n",
    "\n",
    "        # Mean over sequence\n",
    "        x1 = torch.mean(x1, dim=1)\n",
    "        x2 = torch.mean(x2, dim=1)\n",
    "\n",
    "        # Normalization (L2)\n",
    "        x1 = F.normalize(x1, p=2, dim=1)\n",
    "        x2 = F.normalize(x2, p=2, dim=1)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "def triplet_loss_fn(v1, v2, margin=0.25):\n",
    "    scores = torch.matmul(v1, v2.T)\n",
    "    batch_size = scores.size(0)\n",
    "\n",
    "    positive = torch.diag(scores)\n",
    "    negative_without_positive = scores - 2.0 * torch.eye(batch_size, device=scores.device)\n",
    "    closest_negative = negative_without_positive.max(dim=1).values\n",
    "\n",
    "    negative_zero_on_duplicate = scores * (1.0 - torch.eye(batch_size, device=scores.device))\n",
    "    mean_negative = torch.sum(negative_zero_on_duplicate, dim=1) / (batch_size - 1)\n",
    "\n",
    "    triplet_loss1 = torch.maximum(torch.zeros_like(positive), margin - positive + closest_negative)\n",
    "    triplet_loss2 = torch.maximum(torch.zeros_like(positive), margin - positive + mean_negative)\n",
    "\n",
    "    triplet_loss = torch.mean(triplet_loss1 + triplet_loss2)\n",
    "    return triplet_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.01, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for q1_q2 in train_loader:\n",
    "            q1, q2 = q1_q2[0].to(device), q1_q2[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            v1, v2 = model(q1, q2)\n",
    "            loss = triplet_loss_fn(v1, v2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for q1_q2 in val_loader:\n",
    "                q1, q2 = q1_q2[0].to(device), q1_q2[1].to(device)\n",
    "                v1, v2 = model(q1, q2)\n",
    "                val_loss += triplet_loss_fn(v1, v2).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss}\")"
   ],
   "metadata": {
    "id": "4nn8jNXIlsOt"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. data preprocessing"
   ],
   "metadata": {
    "id": "Q45qqbtNDvdd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random as rnd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Cyber_Physical_System/questions.csv\")\n",
    "N_train = 300000\n",
    "N_test  = 10 * 1024\n",
    "data_train = data[:N_train]\n",
    "data_test  = data[N_train:N_train+N_test]\n",
    "del data\n",
    "\n",
    "# Extracting duplicate and non-duplicate question pairs\n",
    "td_index = (data_train['is_duplicate'] == 1).to_numpy()\n",
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "\n",
    "Q1_train_words = np.array(data_train['question1'][td_index])\n",
    "Q2_train_words = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test_words = np.array(data_test['question1'])\n",
    "Q2_test_words = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])\n",
    "\n",
    "# Building the vocabulary\n",
    "from collections import defaultdict\n",
    "Q1_train = np.empty_like(Q1_train_words)\n",
    "Q2_train = np.empty_like(Q2_train_words)\n",
    "\n",
    "Q1_test = np.empty_like(Q1_test_words)\n",
    "Q2_test = np.empty_like(Q2_test_words)\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab['<PAD>'] = 1\n",
    "\n",
    "for idx in range(len(Q1_train_words)):\n",
    "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
    "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
    "    q = Q1_train[idx] + Q2_train[idx]\n",
    "    for word in q:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "\n",
    "\n",
    "# Tokenizing and numerically encoding the questions\n",
    "for idx in range(len(Q1_train)):\n",
    "    Q1_train[idx] = [vocab[word] for word in Q1_train[idx]]\n",
    "    Q2_train[idx] = [vocab[word] for word in Q2_train[idx]]\n",
    "\n",
    "for idx in range(len(Q1_test_words)):\n",
    "    Q1_test[idx] = [vocab[word] for word in nltk.word_tokenize(Q1_test_words[idx])]\n",
    "    Q2_test[idx] = [vocab[word] for word in nltk.word_tokenize(Q2_test_words[idx])]\n",
    "\n",
    "def split_data(Q1, Q2, split_ratio=0.8):\n",
    "    \"\"\" Split the data into training and validation sets. \"\"\"\n",
    "    cut_off = int(len(Q1) * split_ratio)\n",
    "    train_Q1, train_Q2 = Q1[:cut_off], Q2[:cut_off]\n",
    "    val_Q1, val_Q2 = Q1[cut_off:], Q2[cut_off:]\n",
    "    return train_Q1, train_Q2, val_Q1, val_Q2\n",
    "\n",
    "def data_generator(Q1, Q2, batch_size, pad=1):\n",
    "    while True:\n",
    "        for i in range(0, len(Q1), batch_size):\n",
    "            yield (np.array([q + [pad] * (max(map(len, Q1[i:i+batch_size])) - len(q)) for q in Q1[i:i+batch_size]]),\n",
    "                   np.array([q + [pad] * (max(map(len, Q2[i:i+batch_size])) - len(q)) for q in Q2[i:i+batch_size]]))"
   ],
   "metadata": {
    "id": "OVjA5S6xlWOE"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_length = max(max(len(q) for q in Q1_train), max(len(q) for q in Q2_train))\n",
    "\n",
    "Q1_train = [q + [vocab['<PAD>']] * (max_length - len(q)) for q in Q1_train]\n",
    "Q2_train = [q + [vocab['<PAD>']] * (max_length - len(q)) for q in Q2_train]\n",
    "\n",
    "\n",
    "train_Q1, train_Q2, val_Q1, val_Q2 = split_data(Q1_train, Q2_train)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ],
   "metadata": {
    "id": "jsGj7-V9l-nf"
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming train_Q1 and train_Q2 are lists of tokenized questions\n",
    "print(train_Q1[0])  # Print the first question in Q1\n",
    "print(train_Q2[0])  # Print the first question in Q2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRPyE4rnp85D",
    "outputId": "d1b219d7-d927-43cf-85ae-723ba5725d9d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[4, 22, 6, 23, 7, 24, 8, 25, 26, 11, 27, 28, 7, 29, 30, 16, 31, 18, 19, 20, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert to tensors\n",
    "train_Q1_tensor = torch.tensor(train_Q1, dtype=torch.long)\n",
    "train_Q2_tensor = torch.tensor(train_Q2, dtype=torch.long)\n",
    "\n",
    "# Check shapes and types\n",
    "print(\"Shape of train_Q1_tensor:\", train_Q1_tensor.shape)\n",
    "print(\"Shape of train_Q2_tensor:\", train_Q2_tensor.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkwtYNsjp_mp",
    "outputId": "54960c1f-f40c-488f-f888-98688694de5a"
   },
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of train_Q1_tensor: torch.Size([89188, 81])\n",
      "Shape of train_Q2_tensor: torch.Size([89188, 81])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert to tensors\n",
    "val_Q1_tensor = torch.tensor(val_Q1, dtype=torch.long)\n",
    "val_Q2_tensor = torch.tensor(val_Q2, dtype=torch.long)\n",
    "\n",
    "# Check shapes and types\n",
    "print(\"Shape of val_Q1_tensor:\", val_Q1_tensor.shape)\n",
    "print(\"Shape of val_Q2_tensor:\", val_Q2_tensor.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK8MmtaNrQD7",
    "outputId": "5f1766a1-a7b4-4922-c6d7-b841879db7a3"
   },
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of val_Q1_tensor: torch.Size([22298, 81])\n",
      "Shape of val_Q2_tensor: torch.Size([22298, 81])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 DataLoader"
   ],
   "metadata": {
    "id": "joiqErNIEdr0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a simple dataset\n",
    "simple_dataset = TensorDataset(train_Q1_tensor, train_Q2_tensor)\n",
    "\n",
    "# Create a simple DataLoader\n",
    "simple_loader = DataLoader(simple_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Try to get one batch\n",
    "try:\n",
    "    sample_data = next(iter(simple_loader))\n",
    "    print(\"DataLoader works! Sample data shapes:\", [d.shape for d in sample_data])\n",
    "except Exception as e:\n",
    "    print(\"Error with DataLoader:\", e)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xZurEXcqRLw",
    "outputId": "15b4b673-a013-4b69-c77d-048231dacf0f"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataLoader works! Sample data shapes: [torch.Size([256, 81]), torch.Size([256, 81])]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ],
   "metadata": {
    "id": "OFE0kqeTwFM5"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokens_to_sentence(tokenized_sentence, inverse_vocab):\n",
    "    return ' '.join([inverse_vocab[token] for token in tokenized_sentence if token in inverse_vocab])\n",
    "\n",
    "# Example usage\n",
    "tokenized_example = np.array(sample_data[0][0])\n",
    "original_sentence = tokens_to_sentence(tokenized_example, inverse_vocab)\n",
    "print(original_sentence)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8PV6xXLvKUt",
    "outputId": "c11ca139-6b22-4c55-b77f-3bf281de8013"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Would social media have helped Steve Bartman ? <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_example = np.array(sample_data[1][0])\n",
    "original_sentence = tokens_to_sentence(tokenized_example, inverse_vocab)\n",
    "print(original_sentence)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6JZ7hMtr997",
    "outputId": "ab4a4e28-a9d6-486e-fee8-446b1a6a26ce"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "If there had been social media in 2003 , how would the Steve Bartman story been different ? <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 create DataLoader formally"
   ],
   "metadata": {
    "id": "D2epQKCgElBP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming train_Q1 and train_Q2 are lists of tokenized questions\n",
    "train_dataset = TensorDataset(train_Q1_tensor, train_Q2_tensor)\n",
    "val_dataset = TensorDataset(val_Q1_tensor, val_Q2_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# Check the first batch in the train_dataset\n",
    "train_features = next(iter(train_loader))\n",
    "\n",
    "print(\"Shape of train_features Q1:\", train_features[0].shape)\n",
    "print(\"Data type of train_features Q1:\", train_features[0].dtype)\n",
    "print(\"Shape of train_features Q2:\", train_features[1].shape)\n",
    "print(\"Data type of train_features Q2:\", train_features[1].dtype)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0eSYJrXmBVi",
    "outputId": "4a5722bb-b3c5-465a-c042-1c4d05b1fa89"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of train_features Q1: torch.Size([256, 81])\n",
      "Data type of train_features Q1: torch.int64\n",
      "Shape of train_features Q2: torch.Size([256, 81])\n",
      "Data type of train_features Q2: torch.int64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(train_loader))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiTpHTXxxq7F",
    "outputId": "90f467b0-9f2f-44ea-8102-ef5c1ea4f560"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "349\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Instantiate the model and Trainning"
   ],
   "metadata": {
    "id": "UQGFf8ZbEt5L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model = SiameseNetwork(vocab_size=len(vocab), d_model=128)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, learning_rate=0.01, epochs=5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDcMrFaexS9r",
    "outputId": "8956fd94-91c5-4eb6-b1e9-7d4aaa176e35"
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1, Val Loss: 0.06327838808382777\n",
      "Epoch 2, Val Loss: 0.055659063493790614\n",
      "Epoch 3, Val Loss: 0.05284677206707949\n",
      "Epoch 4, Val Loss: 0.05149382970888506\n",
      "Epoch 5, Val Loss: 0.05156079766509885\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the entire model\n",
    "torch.save(model, '/content/model/model_entire.pth')\n",
    "\n",
    "# To load the model later\n",
    "loaded_model = torch.load('/content/model/model_entire.pth')\n",
    "loaded_model.eval()  # Set the model to evaluation mode"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZpueJfH7mkq",
    "outputId": "e6ff7de7-a64c-495c-8e81-adacd0bcec2b"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (embedding): Embedding(41708, 128)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Model Testing"
   ],
   "metadata": {
    "id": "4SRWed5SE2tl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(Q1_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YePXB8pt9UdP",
    "outputId": "20537076-c051-4bce-e234-5e9e91ecfaf5"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[list([32, 38, 4, 107, 65, 1015, 65, 11509, 21])\n",
      " list([30, 156, 78, 216, 8908, 39, 716, 286, 8317, 21])\n",
      " list([32, 38, 4, 521, 1340, 735, 0, 65, 47, 1476, 1341, 735, 21]) ...\n",
      " list([30, 87, 116, 2932, 142, 131, 1747, 2324, 97, 482, 483, 1145, 33, 4075, 21])\n",
      " list([30, 156, 78, 1342, 352, 131, 18010, 21])\n",
      " list([32, 16, 111, 521, 6, 1215, 131, 6170, 21])]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def classify(test_Q1, test_Q2, y, threshold, model, vocab, batch_size=64):\n",
    "    \"\"\"Function to test the accuracy of the model in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions.\n",
    "        y (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (torch.nn.Module): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    accuracy = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_Q1), batch_size):\n",
    "            q1 = torch.tensor(test_Q1[i:i + batch_size], dtype=torch.long).to(device)\n",
    "            q2 = torch.tensor(test_Q2[i:i + batch_size], dtype=torch.long).to(device)\n",
    "            y_test = y[i:i + batch_size]\n",
    "\n",
    "            # Get model predictions\n",
    "            v1, v2 = model(q1, q2)\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(v1, v2)\n",
    "\n",
    "            # Make predictions based on the threshold\n",
    "            predictions = cos_sim > threshold\n",
    "\n",
    "            # Update accuracy\n",
    "            accuracy += torch.sum(predictions == torch.tensor(y_test, dtype=torch.bool)).item()\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    accuracy = accuracy / len(test_Q1)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    return np.array([seq + [pad_value] * (max_len - len(seq)) for seq in sequences])\n",
    "\n",
    "# Determine the maximum length\n",
    "max_length = max(max(len(seq) for seq in Q1_test), max(len(seq) for seq in Q2_test))\n",
    "\n",
    "# Pad the sequences\n",
    "test_Q1_padded = pad_sequences(Q1_test, max_length, vocab['<PAD>'])\n",
    "test_Q2_padded = pad_sequences(Q2_test, max_length, vocab['<PAD>'])\n",
    "\n",
    "# Example usage:\n",
    "accuracy = classify(test_Q1_padded, test_Q2_padded, y_test, 0.7, model, vocab, batch_size=64)\n",
    "print(\"accuracy \", accuracy)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vJf56KD77R_",
    "outputId": "dfc74442-40dd-4b6c-c0f3-307897d08065"
   },
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy  0.70751953125\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Custom testing based on casual text"
   ],
   "metadata": {
    "id": "Lto938QKE65F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import nltk\n",
    "\n",
    "def predict(question1, question2, threshold, model, vocab, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (torch.nn.Module): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Tokenize and numericalize questions\n",
    "    q1 = [vocab.get(word, vocab['<UNK>']) for word in nltk.word_tokenize(question1)]\n",
    "    q2 = [vocab.get(word, vocab['<UNK>']) for word in nltk.word_tokenize(question2)]\n",
    "\n",
    "    # Pad questions\n",
    "    max_len = max(len(q1), len(q2))\n",
    "    q1 += [vocab['<PAD>']] * (max_len - len(q1))\n",
    "    q2 += [vocab['<PAD>']] * (max_len - len(q2))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    q1_tensor = torch.tensor([q1], dtype=torch.long).to(device)\n",
    "    q2_tensor = torch.tensor([q2], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        v1, v2 = model(q1_tensor, q2_tensor)\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(v1, v2)\n",
    "        res = cos_sim.item() > threshold\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Q1 = \", question1)\n",
    "        print(\"Q2 = \", question2)\n",
    "        print(\"Cosine Similarity = \", cos_sim.item())\n",
    "        print(\"Result = \", res)\n",
    "\n",
    "    return res\n",
    "\n",
    "# Example usage\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "predict(question1, question2, 0.7, loaded_model, vocab, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPKVgAiQ-pT3",
    "outputId": "2fc4b40b-5664-4f4b-9ee0-88cddaa84be5"
   },
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q1 =  When will I see you?\n",
      "Q2 =  When can I see you again?\n",
      "Cosine Similarity =  0.724124550819397\n",
      "Result =  True\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When will I see you?\"\n",
    "predict(question1, question2, 0.7, loaded_model, vocab, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhJ9SfW5A_gP",
    "outputId": "77a3e727-d58f-48e2-b17f-12a9078de5c5"
   },
   "execution_count": 76,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q1 =  When will I see you?\n",
      "Q2 =  When will I see you!\n",
      "Cosine Similarity =  0.8320255279541016\n",
      "Result =  True\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "speechOne ='''Halifax is a historic port city and the capital of the Canadian province of Nova Scotia. Nestled on the country's east coast, Halifax is known for its rich maritime heritage and vibrant cultural scene. The city is home to the iconic Halifax Citadel, a star-shaped fortress that offers panoramic views of the harbor. Boasting a diverse population, Halifax is celebrated for its friendly locals and welcoming atmosphere. Visitors can explore its charming waterfront, enjoy fresh seafood, and immerse themselves in the city's lively arts and music scene'''\n",
    "print(speechOne)\n",
    "speechTwo ='''Halifax is a historic port city located in the Canadian province of Nova Scotia. Nestled on the eastern coast of the country, Halifax is renowned for its rich maritime heritage, dating back to its founding in 1749. The city is home to the iconic Citadel Hill, a National Historic Site that offers panoramic views of the harbor and serves as a reminder of Halifax's military past. Boasting a vibrant cultural scene, Halifax hosts numerous festivals, museums, and galleries that celebrate its diverse history and artistic achievements. With its friendly locals, picturesque waterfront, and a thriving culinary scene, Halifax offers a welcoming atmosphere that attracts both residents and visitors alike'''\n",
    "print(speechTwo)\n",
    "speechThree = '''\n",
    "Toronto, the largest city in Canada, is a dynamic and cosmopolitan metropolis situated in the province of Ontario. Known for its striking skyline dominated by the iconic CN Tower, Toronto is a global financial and cultural hub. The city is celebrated for its cultural diversity, reflected in vibrant neighborhoods like Kensington Market and Chinatown, where various ethnic communities thrive. Toronto is also home to world-class attractions such as the Royal Ontario Museum and the Art Gallery of Ontario, contributing to its reputation as a cultural powerhouse. With a bustling downtown core, diverse culinary offerings, and a robust public transportation system, Toronto exemplifies a modern, inclusive urban experience. '''\n",
    "print(speechThree)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTi8MubO_9k3",
    "outputId": "6f55b4fa-b82a-408a-dab3-23501fa6c7e5"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Halifax is a historic port city and the capital of the Canadian province of Nova Scotia. Nestled on the country's east coast, Halifax is known for its rich maritime heritage and vibrant cultural scene. The city is home to the iconic Halifax Citadel, a star-shaped fortress that offers panoramic views of the harbor. Boasting a diverse population, Halifax is celebrated for its friendly locals and welcoming atmosphere. Visitors can explore its charming waterfront, enjoy fresh seafood, and immerse themselves in the city's lively arts and music scene\n",
      "Halifax is a historic port city located in the Canadian province of Nova Scotia. Nestled on the eastern coast of the country, Halifax is renowned for its rich maritime heritage, dating back to its founding in 1749. The city is home to the iconic Citadel Hill, a National Historic Site that offers panoramic views of the harbor and serves as a reminder of Halifax's military past. Boasting a vibrant cultural scene, Halifax hosts numerous festivals, museums, and galleries that celebrate its diverse history and artistic achievements. With its friendly locals, picturesque waterfront, and a thriving culinary scene, Halifax offers a welcoming atmosphere that attracts both residents and visitors alike\n",
      "\n",
      "Toronto, the largest city in Canada, is a dynamic and cosmopolitan metropolis situated in the province of Ontario. Known for its striking skyline dominated by the iconic CN Tower, Toronto is a global financial and cultural hub. The city is celebrated for its cultural diversity, reflected in vibrant neighborhoods like Kensington Market and Chinatown, where various ethnic communities thrive. Toronto is also home to world-class attractions such as the Royal Ontario Museum and the Art Gallery of Ontario, contributing to its reputation as a cultural powerhouse. With a bustling downtown core, diverse culinary offerings, and a robust public transportation system, Toronto exemplifies a modern, inclusive urban experience. \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(speechOne.strip())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJSHO4vtAFUa",
    "outputId": "e1dc9a5d-d3f5-433e-fc47-64ce50ffbbdb"
   },
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Halifax is a historic port city and the capital of the Canadian province of Nova Scotia.\n",
      "-----\n",
      "Nestled on the country's east coast, Halifax is known for its rich maritime heritage and vibrant cultural scene.\n",
      "-----\n",
      "The city is home to the iconic Halifax Citadel, a star-shaped fortress that offers panoramic views of the harbor.\n",
      "-----\n",
      "Boasting a diverse population, Halifax is celebrated for its friendly locals and welcoming atmosphere.\n",
      "-----\n",
      "Visitors can explore its charming waterfront, enjoy fresh seafood, and immerse themselves in the city's lively arts and music scene\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print('\\n-----\\n'.join(sent_detector.tokenize(speechTwo.strip())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1MRyyHAAH1P",
    "outputId": "c79e43d3-7e74-44ba-d737-47cd520fd79e"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Halifax is a historic port city located in the Canadian province of Nova Scotia.\n",
      "-----\n",
      "Nestled on the eastern coast of the country, Halifax is renowned for its rich maritime heritage, dating back to its founding in 1749.\n",
      "-----\n",
      "The city is home to the iconic Citadel Hill, a National Historic Site that offers panoramic views of the harbor and serves as a reminder of Halifax's military past.\n",
      "-----\n",
      "Boasting a vibrant cultural scene, Halifax hosts numerous festivals, museums, and galleries that celebrate its diverse history and artistic achievements.\n",
      "-----\n",
      "With its friendly locals, picturesque waterfront, and a thriving culinary scene, Halifax offers a welcoming atmosphere that attracts both residents and visitors alike\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "speechOneSplit = sent_detector.tokenize(speechOne.strip())\n",
    "speechTwoSplit = sent_detector.tokenize(speechTwo.strip())\n",
    "\n",
    "for i in range(0, len(speechOneSplit)):\n",
    "  question1 = speechOneSplit[i]\n",
    "  question2 = speechTwoSplit[i]\n",
    "  # 1/True means it is duplicated, 0/False otherwise\n",
    "  predict(question1 , question2, 0.7, model, vocab, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzaQFqt5ALxL",
    "outputId": "5786a0ca-ffae-4723-ff99-b48c5e057d62"
   },
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q1 =  Halifax is a historic port city and the capital of the Canadian province of Nova Scotia.\n",
      "Q2 =  Halifax is a historic port city located in the Canadian province of Nova Scotia.\n",
      "Cosine Similarity =  0.882203221321106\n",
      "Result =  True\n",
      "Q1 =  Nestled on the country's east coast, Halifax is known for its rich maritime heritage and vibrant cultural scene.\n",
      "Q2 =  Nestled on the eastern coast of the country, Halifax is renowned for its rich maritime heritage, dating back to its founding in 1749.\n",
      "Cosine Similarity =  0.7190892696380615\n",
      "Result =  True\n",
      "Q1 =  The city is home to the iconic Halifax Citadel, a star-shaped fortress that offers panoramic views of the harbor.\n",
      "Q2 =  The city is home to the iconic Citadel Hill, a National Historic Site that offers panoramic views of the harbor and serves as a reminder of Halifax's military past.\n",
      "Cosine Similarity =  0.8000532984733582\n",
      "Result =  True\n",
      "Q1 =  Boasting a diverse population, Halifax is celebrated for its friendly locals and welcoming atmosphere.\n",
      "Q2 =  Boasting a vibrant cultural scene, Halifax hosts numerous festivals, museums, and galleries that celebrate its diverse history and artistic achievements.\n",
      "Cosine Similarity =  0.6659965515136719\n",
      "Result =  False\n",
      "Q1 =  Visitors can explore its charming waterfront, enjoy fresh seafood, and immerse themselves in the city's lively arts and music scene\n",
      "Q2 =  With its friendly locals, picturesque waterfront, and a thriving culinary scene, Halifax offers a welcoming atmosphere that attracts both residents and visitors alike\n",
      "Cosine Similarity =  0.6771550178527832\n",
      "Result =  False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "speechOneSplit = sent_detector.tokenize(speechOne.strip())\n",
    "speechThreeSplit = sent_detector.tokenize(speechThree.strip())\n",
    "\n",
    "for i in range(0, len(speechOneSplit)):\n",
    "  question1 = speechOneSplit[i]\n",
    "  question2 = speechThreeSplit[i]\n",
    "  # 1/True means it is duplicated, 0/False otherwise\n",
    "  predict(question1 , question2, 0.7, model, vocab, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNoYIbcBALiR",
    "outputId": "0df7db69-8595-46bf-fb08-856f1312df41"
   },
   "execution_count": 73,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q1 =  Halifax is a historic port city and the capital of the Canadian province of Nova Scotia.\n",
      "Q2 =  Toronto, the largest city in Canada, is a dynamic and cosmopolitan metropolis situated in the province of Ontario.\n",
      "Cosine Similarity =  0.42908966541290283\n",
      "Result =  False\n",
      "Q1 =  Nestled on the country's east coast, Halifax is known for its rich maritime heritage and vibrant cultural scene.\n",
      "Q2 =  Known for its striking skyline dominated by the iconic CN Tower, Toronto is a global financial and cultural hub.\n",
      "Cosine Similarity =  0.4907105267047882\n",
      "Result =  False\n",
      "Q1 =  The city is home to the iconic Halifax Citadel, a star-shaped fortress that offers panoramic views of the harbor.\n",
      "Q2 =  The city is celebrated for its cultural diversity, reflected in vibrant neighborhoods like Kensington Market and Chinatown, where various ethnic communities thrive.\n",
      "Cosine Similarity =  0.5867418646812439\n",
      "Result =  False\n",
      "Q1 =  Boasting a diverse population, Halifax is celebrated for its friendly locals and welcoming atmosphere.\n",
      "Q2 =  Toronto is also home to world-class attractions such as the Royal Ontario Museum and the Art Gallery of Ontario, contributing to its reputation as a cultural powerhouse.\n",
      "Cosine Similarity =  0.07158582657575607\n",
      "Result =  False\n",
      "Q1 =  Visitors can explore its charming waterfront, enjoy fresh seafood, and immerse themselves in the city's lively arts and music scene\n",
      "Q2 =  With a bustling downtown core, diverse culinary offerings, and a robust public transportation system, Toronto exemplifies a modern, inclusive urban experience.\n",
      "Cosine Similarity =  0.498212993144989\n",
      "Result =  False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "question3 = \"The Great Wall of China, a marvel of engineering stretching over 13,000 miles. is a series of fortifications made of stone, brick, tamped earth, wood, and other materials.\"\n",
    "question4 = \"a The great or China marvel of engineering stretching over 13,000 miles is\"\n",
    "predict(question3, question4, 0.7, loaded_model, vocab, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gU6T2Xf1AYmc",
    "outputId": "45eae8ac-f639-47af-c9ae-c2fe77aa9efb"
   },
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q1 =  The Great Wall of China, a marvel of engineering stretching over 13,000 miles. is a series of fortifications made of stone, brick, tamped earth, wood, and other materials.\n",
      "Q2 =  a The great or China marvel of engineering stretching over 13,000 miles is\n",
      "Cosine Similarity =  0.7929869890213013\n",
      "Result =  True\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ]
  }
 ]
}
